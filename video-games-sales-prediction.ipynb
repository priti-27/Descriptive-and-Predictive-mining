{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is an attempt to predict the sales of future games using data scraped from VGChartz and  Metacritic available on Kaggle.\n",
    "The predictive models are only trained on North America sales at the moment. \n",
    "Comments and suggestions are more than welcomed.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "library(reshape2)\n",
    "library(dplyr)\n",
    "library(plotly)\n",
    "library(caret)\n",
    "library(corrplot)\n",
    "library(RColorBrewer)\n",
    "vg <-  read.csv(\"../input/Video_Games_Sales_as_at_22_Dec_2016.csv\")\n",
    "vg$Year_of_Release <- as.numeric(as.character(vg$Year_of_Release))\n",
    "vg$User_Score <- as.numeric(as.character(vg$User_Score))\n",
    "vg[vg==\"\"] <-NA\n",
    "vg$Genre <- as.character(vg$Genre)\n",
    "vg$Genre[vg$Genre==\"Role-Playing\"] <-\"RolePlaying\"\n",
    "vg <- vg %>% filter(vg$Year_of_Release<=2016)\n",
    "vg <- vg%>% arrange(desc(Year_of_Release))\n",
    "```\n",
    "\n",
    "## Descriptive analysis\n",
    "\n",
    "The goal here is to gain some intuitions of the magnitude of video game sales and its evolution throughout the period of time covered by the data set.\n",
    "\n",
    "The following three graphs present the number of games released in each year, the total units sold of games released in each year and the average units sold per game of games released in each year.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "NR <- vg%>% select(Year_of_Release) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Count=n())\n",
    "\n",
    "plot_ly(data=NR,x=~Year_of_Release)%>%\n",
    "  add_trace(y=~Count,name=\"Number of release\",mode=\"lines\",type = 'scatter',\n",
    "color = '#000000') %>%\n",
    "layout(title = \"Fig.1 Number of games released per year\",\n",
    "    yaxis = list(title=\"Games released (count)\"))\n",
    "\n",
    "```\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "Sales_NA <- vg%>% select(Year_of_Release,NA_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Sum_NA_Sales=sum(NA_Sales))\n",
    "Sales_EU <- vg%>% select(Year_of_Release,EU_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Sum_EU_Sales=sum(EU_Sales))\n",
    "Sales_JP <- vg%>% select(Year_of_Release,JP_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Sum_JP_Sales=sum(JP_Sales))\n",
    "Sales_OH <- vg%>% select(Year_of_Release,Other_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Sum_OH_Sales=sum(Other_Sales))\n",
    "\n",
    "Sales_evo <- Reduce(function(x,y) merge(x,y,all=TRUE,by=\"Year_of_Release\"),list(Sales_NA,Sales_EU,Sales_JP,Sales_OH))\n",
    "\n",
    "plot_ly(data=Sales_evo,x=~Year_of_Release)%>%\n",
    "  add_trace(y=~Sum_NA_Sales,name=\"North America Sales\",mode=\"lines\",type = 'scatter') %>%\n",
    "  add_trace(y=~Sum_EU_Sales,name=\"Europe Sales\",mode=\"lines\",type = 'scatter') %>%\n",
    "  add_trace(y=~Sum_JP_Sales,name=\"Japan Sales\",mode=\"lines\",type = 'scatter') %>%\n",
    "  add_trace(y=~Sum_OH_Sales,name=\"Other Sales\",mode=\"lines\",type = 'scatter') %>%\n",
    "  layout(title = \"Fig.2 Total units sold by year of release and by region\",\n",
    "  yaxis = list(title=\"Sales (in millions of units)\"))\n",
    " ```\n",
    "\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "MSales_NA <- vg%>% select(Year_of_Release,NA_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Mean_NA_Sales=mean(NA_Sales),SD_NA_Sales =sd(NA_Sales))\n",
    "MSales_EU <- vg%>% select(Year_of_Release,EU_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Mean_EU_Sales=mean(EU_Sales),SD_EU_Sales =sd(EU_Sales))\n",
    "MSales_JP <- vg%>% select(Year_of_Release,JP_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Mean_JP_Sales=mean(JP_Sales),SD_JP_Sales =sd(JP_Sales))\n",
    "MSales_OH <- vg%>% select(Year_of_Release,Other_Sales) %>%\n",
    "  group_by(Year_of_Release)%>% \n",
    "  summarise(Mean_OH_Sales=mean(Other_Sales),SD_OH_Sales =sd(Other_Sales))\n",
    "\n",
    "MSales_evo <- Reduce(function(x,y) merge(x,y,all=TRUE,by=\"Year_of_Release\"),list(MSales_NA,MSales_EU,MSales_JP,MSales_OH))\n",
    "\n",
    "plot_ly(data=MSales_evo,x=~Year_of_Release)%>%\n",
    "  add_trace(y=~Mean_NA_Sales,name=\"North America Sales\",\n",
    "            mode=\"lines\",type = 'scatter',\n",
    "            error_y = ~list(value = SD_NA_Sales,\n",
    "                            color = '#000000')) %>%\n",
    "  add_trace(y=~Mean_EU_Sales,name=\"Europe Sales\",\n",
    "            mode=\"lines\",type = 'scatter',\n",
    "            error_y = ~list(value = SD_EU_Sales,\n",
    "                            color = '#000000')) %>%\n",
    "  add_trace(y=~Mean_JP_Sales,name=\"Japan Sales\",\n",
    "            mode=\"lines\",type = 'scatter',\n",
    "            error_y = ~list(value = SD_JP_Sales,\n",
    "                            color = '#000000')) %>%\n",
    "  add_trace(y=~Mean_OH_Sales,name=\"Other Sales\",\n",
    "            mode=\"lines\",type = 'scatter',\n",
    "            error_y = ~list(value = SD_OH_Sales,\n",
    "                            color='#000000'))%>%\n",
    "    layout(title = \"Fig.3 Average units sold per game by year of release and by region\",\n",
    "    yaxis = list(title=\"Sales (in millions of units)\"))\n",
    "```\n",
    "\n",
    "From Figure 1, we can see that the number of games released reached its peak at around 2008-2009. This peak also corresponds to a peak in the total units of games sold as shown in Figure 2. \n",
    "However, in terms of the average units sold per game, we do not observe any particular increase during this period in Figure 3. \n",
    "In fact, the average units sold per game has greatly declined from 1980s to 1990s. This is probably due to the increased competition in the market. \n",
    "This number became more stable in the 2000s before experiencing a slight decrease again starting from 2013.\n",
    "\n",
    "The finding that the average units sold per game is not stationnary throughout the period covered has some implications on predictive modelling that will be discussed in the following paragraph.\n",
    "\n",
    "The following graph presents the percentage of the total sales by year of release that different game platforms shared. \n",
    "Game platforms are represented by colors from different palettes according to their vendor. I took the platform-vendor definition from [Leonard's analysis](https://www.kaggle.com/leonardf/releases-and-sales).\n",
    "Microsoft platforms are colored in greys, Nintendo in blues, Sega in oranges, Sony in greens, other platforms (including PC) are colored in purples.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "nintendoplatforms = c(\"3DS\",\"DS\",\"GB\",\"GBA\",\"N64\",\"GC\", \"NES\",\"SNES\",\"Wii\",\"WiiU\")\n",
    "sonyplatforms = c(\"PS\",\"PS2\",\"PSP\",\"PS3\",\"PS4\",\"PSV\")\n",
    "segaplatforms = c(\"GEN\",\"SCD\",\"DC\",\"GG\",\"SAT\")\n",
    "msplatforms = c(\"XB\",\"X360\", \"XOne\")\n",
    "otherplatforms = c(\"2600\",\"3DO\",\"NG\",\"PCFX\",\"TG16\",\"WS\")\n",
    "pc= c('PC')\n",
    "\n",
    "vg$Platformvendor[vg$Platform %in% nintendoplatforms] <- \"Nintendo\"\n",
    "vg$Platformvendor[vg$Platform %in% sonyplatforms] <- \"Sony\"\n",
    "vg$Platformvendor[vg$Platform %in% msplatforms] <- \"Microsoft\"\n",
    "vg$Platformvendor[vg$Platform %in% segaplatforms] <- \"Sega\"\n",
    "vg$Platformvendor[vg$Platform %in% pc] <- \"PC\"\n",
    "vg$Platformvendor[vg$Platform %in% otherplatforms] <- \"Other\"\n",
    "\n",
    "Platform_level <- vg%>% group_by(Platform)%>% \n",
    "  summarise(Sales = sum(Global_Sales))\n",
    "\n",
    "Platform_level<- left_join(Platform_level,vg[,c(\"Platformvendor\",\"Platform\")])\n",
    "Platform_level <- unique(Platform_level)\n",
    "Platform_level<- Platform_level%>% arrange(Platformvendor,Sales)\n",
    "Platform_level$Platform <-factor(Platform_level$Platform , levels = Platform_level$Platform)\n",
    "\n",
    "Platform_level$color <- c(brewer.pal(3,'Greys'),\n",
    "                          brewer.pal(9,'Blues'),\"#000000\",\n",
    "                          brewer.pal(7,'Purples'),\n",
    "                          brewer.pal(5,'Oranges'),\n",
    "                          brewer.pal(6,'Greens'))\n",
    "                          \n",
    "N_platform <-vg%>%group_by(Platform,Year_of_Release) %>% \n",
    "  summarise(Sales=sum(Global_Sales))\n",
    "\n",
    "N_platform<- N_platform%>%group_by(Year_of_Release)%>% \n",
    "  mutate(YearTotal = sum(Sales),Percent = Sales/YearTotal)\n",
    "N_platform$Platform <-factor(N_platform$Platform , levels = Platform_level$Platform)\n",
    "\n",
    "plot_ly(N_platform,x=~Year_of_Release,y=~Percent,\n",
    "          color=~Platform,colors=Platform_level$color,\n",
    "          hoverinfo='text',type='bar',\n",
    "          text=~paste('Year : ', Year_of_Release,'<br>',\n",
    "                      'Platform : ',Platform,'<br>',\n",
    "                      'Sales (in millions of units): ', Sales,'<br>',\n",
    "                      'Percentage in total game sales: ',round(Percent,2)*100,'%'))%>%\n",
    "    layout(barmode='stack',\n",
    "    title='Fig.4 Percentage of game platforms in total sales by year of release')\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "## Predictive analysis\n",
    "\n",
    "The goal now is to predict the sales of future games based on the pattern that can be learned from this data set. At the moment, the analysis focuses only on the sales in North America.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "D_model <-  vg[,c('NA_Sales',\n",
    "                'Name',\n",
    "                'Year_of_Release',\n",
    "                'Publisher',\n",
    "                'Platform',\n",
    "                'Genre',\n",
    "                'Critic_Score',\n",
    "                'Critic_Count',\n",
    "                'User_Score',\n",
    "                'User_Count')]\n",
    "```\n",
    "### Data preprocessing\n",
    "\n",
    "#### Data imputation and data splitting\n",
    "\n",
    "One of the difficulties this data set presents is the prevalence of missing values. In fact, the values for Critic Score, Critic Count, User Score and User Count are missing for more than half of the observations in the data set. \n",
    "The following shows the percentage of missing values in each variable.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "naPerc<-apply(D_model,2, function(x){length(which(is.na(x)))/dim(D_model)[1]})\n",
    "naPerc\n",
    "```\n",
    "\n",
    "For variable with few missing values such as Name and Genre, we can simply delete the observations with missing values without much consequence. For other variables where there is a large percentage of missing values, bagged decision tree is used to impute these values.\n",
    "\n",
    "To make the analysis more realistic, I used the older 80% of the observations to train the models, the following 10% to choose the hyperparameters and the most recent 10% to test the models. \n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "# delete observations w missing val in Genre & Name\n",
    "D_model <- D_model %>% filter(!is.na(Genre)&!is.na(Name))\n",
    "# impute data for other predictor with missing value\n",
    "# keep the more recent 10% as test data\n",
    "preproc <- preProcess(D_model[,-1], method = c(\"bagImpute\")) # alternative :knnImpute, median\n",
    "X_train <- predict(preproc, D_model[D_model$Year_of_Release <= quantile(D_model$Year_of_Release, .9),-1])\n",
    "X_test <- predict(preproc, D_model[D_model$Year_of_Release > quantile(D_model$Year_of_Release, .9),-1])\n",
    "Y_test <- D_model[1:dim(X_test)[1],1]\n",
    "length(Y_test)==dim(X_test[1])[1]\n",
    "Y_train <- D_model[dim(X_test)[1]+1:dim(X_train)[1],1]\n",
    "length(Y_train)==dim(X_train[1])[1]\n",
    "D_train <- cbind(Y_train,X_train)\n",
    "train_indx <- which(D_train$Year_of_Release<quantile(D_train$Year_of_Release,0.875))\n",
    "```\n",
    "\n",
    "#### Feature engineering\n",
    "\n",
    "Publisher should be an important factor that influences the sales of a game. However, this variable can not be used directly in the models for two reasons. \n",
    "First, this categorical variable contains far too many categories (all publishers ever existed in the data set since the 80s). Second, there will possibly be some publishers in the test set that have never been seen by the models in training.\n",
    "To create a variable that captures the influence of publisher on games sales, I used the average units sold per game over the last four years of each publisher in the training set. \n",
    "This value will be 0 if a publisher has not published any game in the last four years. The reason for using only the sales in the training set is to avoid data snooping.\n",
    "For publishers in the test set that do not exist in the training set, the value of this variable will also be zero.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "P_avg_Sales  <-D_train%>%\n",
    "  filter(Year_of_Release>=max(Year_of_Release)-3)%>% \n",
    "  group_by(Publisher)%>% \n",
    "  mutate(avgSales = mean(Y_train))%>%ungroup()%>%\n",
    "  select(Publisher,avgSales)%>% unique()\n",
    "  \n",
    "D_train<- left_join(D_train,P_avg_Sales,by=\"Publisher\")\n",
    "D_train[is.na(D_train)]<-0\n",
    "D_train <- D_train%>% select(-Publisher,-Name)\n",
    "X_test <- left_join(X_test,P_avg_Sales,by=\"Publisher\")\n",
    "X_test[is.na(X_test)]<-0\n",
    "X_test <- X_test%>% select(-Publisher,-Name)\n",
    "```\n",
    "#### Centering and scaling\n",
    "\n",
    "Finally, I centered and scaled all the numerical variables (except Year of Release).\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "cs <- preProcess(D_train[,c(5:9)],method=c(\"center\",\"scale\"))\n",
    "D_train <- cbind(D_train[,c(1:4)],predict(cs,D_train[,c(5:9)]))\n",
    "cs <- preProcess(X_test[,c(4:8)],method=c(\"center\",\"scale\"))\n",
    "X_test <- cbind(X_test[,c(1:3)],predict(cs,X_test[,c(4:8)]))\n",
    "```\n",
    "\n",
    "Here is how the final data that will be used to train the models looks like\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "head(D_train)\n",
    "```\n",
    "As as sanity check, the predictors are not highly correlated.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "cormax <- cor(D_train[,c(5:9)])\n",
    "corrplot(cormax,method=\"number\")\n",
    "```\n",
    "\n",
    "### Modelling\n",
    "\n",
    "As mentionned earlier, the average units sold per game has evolved greatly since the 80s when the data set begins. This poses another difficulty to predictive modelling since models trained on the older data might generalize poorly to the newer data used for testing. Besides, the validation result might not be a good indication of the test result under these conditions.\n",
    "Some of my attempts to tackle this problem include attaching more weights to recent observations during training and choosing models that are robust to outliers such as support vector machine and random forest. \n",
    "\n",
    "The four models evaluated here are linear regression, elastic net, support vector machine and random forest.\n",
    "\n",
    "#### Linear regression\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "ctrl <- trainControl(method='LGOCV',\n",
    "                 index = list(TrainSet = train_indx))\n",
    "                 set.seed(666)\n",
    "M_base <- train(Y_train~Year_of_Release+Genre+Critic_Score+\n",
    "          Critic_Count+User_Score+User_Count+Platform,\n",
    "          weights=exp(D_train$Year_of_Release-min(D_train$Year_of_Release)+1),# more weights for recent data\n",
    "          data=D_train,\n",
    "        trControl=ctrl,\n",
    "        method=\"lm\")\n",
    "train_p_b <- predict(M_base,D_train)\n",
    "test_p_b <- predict(M_base,X_test)\n",
    "RMSE(train_p_b, D_train$Y_train) # training error\n",
    "M_base$results['RMSE'] # validation error\n",
    "```\n",
    "#### Elastic net\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "glmnetgrid <-expand.grid(alpha=c(0.1,0.55,1),lambda=seq(0,0.5,0.1))\n",
    "set.seed(666)\n",
    "M_glmnet<- train(Y_train~Year_of_Release+Genre+Critic_Score+\n",
    "                  Critic_Count+User_Score+User_Count+Platform+avgSales,\n",
    "                weights=exp(D_train$Year_of_Release-min(D_train$Year_of_Release)+1),# more weights for recent data\n",
    "                data=D_train,\n",
    "                trControl=ctrl,\n",
    "                method=\"glmnet\",\n",
    "                tuneGrid=glmnetgrid)\n",
    "train_p_glmnet <- predict(M_glmnet,D_train)\n",
    "test_p_glmnet<- predict(M_glmnet,X_test)\n",
    "RMSE(train_p_glmnet, D_train$Y_train) # training error\n",
    "M_glmnet$results[row.names(M_glmnet$bestTune),'RMSE']# validation error\n",
    "```\n",
    "#### Support vector machine\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "set.seed(666)\n",
    "M_svm <- train(Y_train~Year_of_Release+Genre+Critic_Score+\n",
    "              Critic_Count+User_Score+User_Count+Platform+avgSales,\n",
    "               data=D_train,\n",
    "              weights=exp(D_train$Year_of_Release-min(D_train$Year_of_Release)+1),\n",
    "               method=\"svmRadial\",\n",
    "               trControl=ctrl)\n",
    "train_p_svm <- predict(M_svm,D_train)\n",
    "test_p_svm<- predict(M_svm,X_test)\n",
    "RMSE(train_p_svm, D_train$Y_train) # training error\n",
    "M_svm$results[row.names(M_svm$bestTune),'RMSE']# validation error\n",
    "```\n",
    "\n",
    "#### Random forest\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "\n",
    "set.seed(666)\n",
    "M_rf <- train(Y_train~Year_of_Release+Genre+Critic_Score+\n",
    "                 Critic_Count+User_Score+User_Count+Platform+avgSales,\n",
    "              weights=exp(D_train$Year_of_Release-min(D_train$Year_of_Release)+1),# more weights for recent data\n",
    "              data=D_train,\n",
    "               method=\"rf\",\n",
    "               tuneLength=2,\n",
    "              trControl=ctrl)\n",
    "train_p_rf <- predict(M_rf,D_train)\n",
    "test_p_rf<- predict(M_rf,X_test)\n",
    "RMSE(train_p_rf, D_train$Y_train) # training error\n",
    "M_rf$results[row.names(M_rf$bestTune),'RMSE']# validation error\n",
    "```\n",
    "### Model selection\n",
    "\n",
    "The following graph puts together the validation result of the four models for comparaison. Note that only one validation set is used here, namely the 10% second most recent data. \n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "modelList <- list(lm=M_base,\n",
    "                  enet =M_glmnet,\n",
    "                  svm=M_svm,\n",
    "                  rf=M_rf)\n",
    "resamps<- resamples(modelList)\n",
    "bwplot(resamps)\n",
    "```\n",
    "\n",
    "Support vector machine and elastic net are the two models with the best performance. Their performances are actually very close to each other. \n",
    "Random forest has a slightly worse performance compared to the two models. Linear regression has the worst performance. \n",
    "\n",
    "### Model testing\n",
    "\n",
    "Let's now test the four models on the test set and hope that the models that did well on the validation set will also do well on the test set.\n",
    "\n",
    "It is also helpful to compare the performance of the models to a baseline performance. \n",
    "Here, a naive approach to predict the future sales would be to use the average past sales. This gives us a baseline test performance of root mean squared error 0.424387. Our models should at least perform better than this.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "train_mean <- mean(D_train$Y_train)\n",
    "RMSE(rep(train_mean,length(Y_test)), Y_test) \n",
    "```\n",
    "\n",
    "Below are the test performance of the four models. All of the four models have lower root mean squared error than the baseline. This is a good sign.\n",
    "The test performance of the four models are also ranked in a similar way as their training performance. \n",
    "Support vector machine therefore appears to be the best model for this given data (for now).\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "RMSE(test_p_b, Y_test) #linear regression\n",
    "RMSE(test_p_rf, Y_test) # random forest\n",
    "RMSE(test_p_glmnet, Y_test) # elastic net\n",
    "RMSE(test_p_svm, Y_test) # support vector machine\n",
    "```\n",
    "\n",
    "\n",
    "### Model stacking\n",
    "\n",
    "Instead of choosing just one single best performing model using the validation results, we can also take the average of the predictions of the four models as the final predictions.\n",
    "\n",
    "```{r, warning=FALSE,message=FALSE}\n",
    "test_stacked <-(test_p_b+test_p_glmnet+test_p_rf+test_p_svm)/4\n",
    "RMSE(test_stacked, Y_test)\n",
    "```\n",
    "Here, model stacking does not give us a better result compared to using support vector machine alone. \n",
    "However, stacking multiple different models makes the predictions more stable and potentially more robust to future changes in the data distribution.\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "* Feature engineering \n",
    "* Explore other predictive models\n",
    "* Alternative way of model stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}